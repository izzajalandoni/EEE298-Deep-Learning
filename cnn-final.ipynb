{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 86,410\n",
      "Trainable params: 86,154\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Augmenting data.\n",
      "Epoch 1/50\n",
      "390/390 [==============================] - 12s 30ms/step - loss: 1.6761 - acc: 0.3878 - val_loss: 1.6566 - val_acc: 0.4080\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 1.4033 - acc: 0.4921 - val_loss: 1.3750 - val_acc: 0.5001\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 1.2801 - acc: 0.5400 - val_loss: 1.3058 - val_acc: 0.5240\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 1.2035 - acc: 0.5714 - val_loss: 1.3182 - val_acc: 0.5452\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 1.1439 - acc: 0.5925 - val_loss: 1.1718 - val_acc: 0.5924\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 1.1043 - acc: 0.6070 - val_loss: 1.3252 - val_acc: 0.5498\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 1.0645 - acc: 0.6232 - val_loss: 1.0461 - val_acc: 0.6404\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 1.0434 - acc: 0.6331 - val_loss: 1.1141 - val_acc: 0.6156\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 1.0102 - acc: 0.6432 - val_loss: 1.1893 - val_acc: 0.5995\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.9929 - acc: 0.6489 - val_loss: 0.9897 - val_acc: 0.6510\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.9680 - acc: 0.6578 - val_loss: 1.0718 - val_acc: 0.6288\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.9625 - acc: 0.6595 - val_loss: 1.0132 - val_acc: 0.6449\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.9408 - acc: 0.6678 - val_loss: 1.4798 - val_acc: 0.5672\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.9303 - acc: 0.6746 - val_loss: 1.4772 - val_acc: 0.5696\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.9206 - acc: 0.6776 - val_loss: 0.9941 - val_acc: 0.6600\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8998 - acc: 0.6815 - val_loss: 1.0824 - val_acc: 0.6238\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.8953 - acc: 0.6854 - val_loss: 0.9007 - val_acc: 0.6966\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8834 - acc: 0.6891 - val_loss: 1.0213 - val_acc: 0.6451\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8721 - acc: 0.6929 - val_loss: 0.8985 - val_acc: 0.6902\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8633 - acc: 0.6961 - val_loss: 0.9381 - val_acc: 0.6865\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8581 - acc: 0.7011 - val_loss: 0.9989 - val_acc: 0.6571\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8527 - acc: 0.7008 - val_loss: 0.9215 - val_acc: 0.6851\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8434 - acc: 0.7063 - val_loss: 0.8964 - val_acc: 0.7000\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.8343 - acc: 0.7056 - val_loss: 1.1338 - val_acc: 0.6301\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8302 - acc: 0.7101 - val_loss: 0.8358 - val_acc: 0.7107\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8280 - acc: 0.7094 - val_loss: 0.8735 - val_acc: 0.6965\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.8130 - acc: 0.7149 - val_loss: 0.9469 - val_acc: 0.6844\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8156 - acc: 0.7152 - val_loss: 0.8756 - val_acc: 0.6993\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.8001 - acc: 0.7189 - val_loss: 0.9635 - val_acc: 0.6733\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7966 - acc: 0.7207 - val_loss: 1.0409 - val_acc: 0.6518\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 11s 29ms/step - loss: 0.7928 - acc: 0.7234 - val_loss: 0.8247 - val_acc: 0.7182\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7897 - acc: 0.7237 - val_loss: 0.7997 - val_acc: 0.7336\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7899 - acc: 0.7248 - val_loss: 0.8102 - val_acc: 0.7241\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7815 - acc: 0.7279 - val_loss: 0.8470 - val_acc: 0.7099\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7801 - acc: 0.7268 - val_loss: 0.9925 - val_acc: 0.6808\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 11s 29ms/step - loss: 0.7755 - acc: 0.7280 - val_loss: 0.8455 - val_acc: 0.7115\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7643 - acc: 0.7343 - val_loss: 1.1220 - val_acc: 0.6422\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7585 - acc: 0.7359 - val_loss: 0.8993 - val_acc: 0.6925\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.7552 - acc: 0.7357 - val_loss: 0.9727 - val_acc: 0.6671\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.7624 - acc: 0.7332 - val_loss: 0.8462 - val_acc: 0.7047\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.7556 - acc: 0.7361 - val_loss: 0.7748 - val_acc: 0.7338\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7523 - acc: 0.7378 - val_loss: 0.8681 - val_acc: 0.7065\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7469 - acc: 0.7401 - val_loss: 0.7635 - val_acc: 0.7398\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7430 - acc: 0.7411 - val_loss: 0.8377 - val_acc: 0.7114\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 11s 27ms/step - loss: 0.7399 - acc: 0.7405 - val_loss: 0.7675 - val_acc: 0.7375\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7401 - acc: 0.7404 - val_loss: 0.7529 - val_acc: 0.7423\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7385 - acc: 0.7424 - val_loss: 0.7886 - val_acc: 0.7259\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7345 - acc: 0.7435 - val_loss: 0.7689 - val_acc: 0.7378\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7316 - acc: 0.7448 - val_loss: 0.7888 - val_acc: 0.7349\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 11s 28ms/step - loss: 0.7285 - acc: 0.7473 - val_loss: 0.7503 - val_acc: 0.7406\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Test Accuracy: 74%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Title: CNN CIFAR10 dataset\n",
    "# Author: Izza Claire M. Jalandoni\n",
    "# Created: March 2, 2019 using Google Colab\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Input, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data() # loading datasets\n",
    "num_labels = len(np.unique(y_train))                    # computing number labels\n",
    "num_samples = len(x_train)\n",
    "\n",
    "#print(\"x_train shape:\", x_train.shape)\n",
    "#print(x_train.shape[0],\"train samples\")\n",
    "#print(\"Number of labels: %d\" % num_labels)\n",
    "\n",
    "y_train = to_categorical(y_train,num_labels)\n",
    "y_test = to_categorical(y_test,num_labels)\n",
    "\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "\n",
    "# set hyperparameters\n",
    "data_augmentation = True\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "filters = 64\n",
    "dropout = 0.2\n",
    "epochs = 50\n",
    "\n",
    "steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Observations:\n",
    "# 1. Adam best optimizer but converges fast, increasing epochs only encourage overfitting\n",
    "# 2. Adding dropout in other layers decreases accuracy\n",
    "# 3. Trying to add batch normalization, adding increased to 79%\n",
    "\n",
    "# using sequential to build network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters, (kernel_size, kernel_size),\n",
    "                activation='relu',input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters, kernel_size=kernel_size,\n",
    "                activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters, kernel_size=kernel_size,\n",
    "                activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "plot_model(model,to_file='cnn-cifar.png',show_shapes=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "if data_augmentation:\n",
    "  print(\"Augmenting data.\")\n",
    "  datagen = ImageDataGenerator(\n",
    "    featurewise_center = False, # set input mean to 0 over dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,\n",
    "    samplewise_std_normalization = False,\n",
    "    zca_whitening = False,\n",
    "    zca_epsilon = 1e-06,\n",
    "    rotation_range = 0,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0,\n",
    "    zoom_range = 0,\n",
    "    channel_shift_range = 0,\n",
    "    fill_mode = 'nearest',\n",
    "    cval = 0,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    "    rescale = None,\n",
    "    preprocessing_function = None,\n",
    "    data_format = None,\n",
    "    validation_split = 0.0)\n",
    "\n",
    "  datagen.fit(x_train)\n",
    "  history = model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),\n",
    "                      epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                      validation_data=(x_test,y_test), workers=4)\n",
    "\n",
    "else:\n",
    "  print(\"Not augmenting data.\")\n",
    "  history = model.fit(x_train,y_train,epochs=epochs,validation_split=0.3,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "loss,acc = model.evaluate(x_test,y_test,batch_size=batch_size)\n",
    "print(\"Test Accuracy: %.lf%%\" % (100.0*acc))\n",
    "\n",
    "# priting history\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
